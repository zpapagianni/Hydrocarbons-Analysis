---
title: "Hydrocarbon Analysis"
subtitle: "Exploratory analysis and Prediction"
output:
  html_document:
    toc: true
    df_print: paged

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(fig.align = 'center')
# Load relevant packages -------------------------------------------------------
library(dplyr)
library(irlba)
library(ggplot2)
library(ggpubr)
library(glmnet)
library(tidyverse)
library(reshape2)
library(randomForest)
library(e1071)
library(caret)
library(Metrics)
library(tree)
library(pls)
library(gridExtra)
library(corrplot)
library(Hmisc)
library(PerformanceAnalytics)
library(gbm)

#Set seed to replicate results
set.seed(9)
#Functions --------------------------------------------------------------------
#This formats a correlation matrix into a table with 4 columns containing
#row names,column names, the correlation coefficients,the p-values of the correlations
#source(http://www.sthda.com/english/wiki/correlation-matrix-formatting-and-visualization)
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}
```

# Introduction

This study takes a data-driven approach to explore the degree to which the reactor calibration settings and composition of the feed hydrocarbon mixed influence the yield of hydrocarbon in a target range of densities. 

Hydrocarbons are chemical compounds with molecules consisting of hydrogen and carbon atoms. Hydrocracking is an industrial process in which hydrocarbons with molecules made of long chains of atoms are broken down into smaller molecules. This is done by the addition of hydrogen at high pressures and temperatures in the presence of a chemical catalyst. This process is used in the petrochemical industry to increase the proportion of extracted hydrocarbons that have shorter molecules, which are typically more useful to consumers and attract a higher profit on sale.

The process of hydrocracking can be controlled by adjusting the temperature of the reactor (or equivalently its pressure), the catalyst that is used and the time for which the hydrocarbon mixture is within the reactor. The composition of the effluent hydrocarbon mixture will depend on these settings of the reactor and on the composition of the feed hydrocarbon mixture that entered the reactor.
The most useful and valuable hydrocarbons are not the heaviest (most dense, with longest molecules) or the lightest (least dense, with smallest molecules), but those within a density in a specific, intermediate range. This is known as the target range.

This study uses a qualitative and quantitative approach to assess which of their control variables are most influential on the yield of hydrocarbon in the target range of densities. We will also predict the yield in this range for a given feed composition and reactor calibration. This would allow to not only maximise profits, but to minimise waste by calibrating reactor output to meet but not exceed demand.

# Exploratory analysis of the data

## Structure 
The data contains observations for 497 days of 44 variables, which detail the reactor settings and feed composition.In particular:

* Date: Dates from 15/10/2020 to 23/02/2022.
* Catalyst:  Which of 3 catalysts were used. Can be 0,1 or 2.
* Temperature:  Reactor temperature in degrees Fahrenheit.
* Through time: Reactor residence time of the mixture in hours. 
* feed fraction:Composition of the feed hydrocarbon mixtures as the proportion of the overall mass in each of 20 density intervals.
* out fraction:Composition of the effluent hydrocarbon mixtures as the proportion of the overall mass in each of 20 density intervals

```{r,echo=FALSE}
# Load data --------------------------------------------------------------------
hydro<- read_csv("data.csv") 
head(hydro,5)  %>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

The variable `date` is of no use for the purposes of this study, since it doesn’t affect the effluent mixture.The variable `catalyst` is categorical in nature and not numerical. The density intervals are ranging from 1 to 20.Interval 1 corresponds to the longest, heaviest molecules and interval 20 to the shortest,lightest molecules. This study is targeting the density ranges 13, 14, and 15, which we will refer to as the response variables.

Next, we will proceed to create the final data frame by droping the column date and converting the variable catalyst to factor.
```{r}
#Clean data --------------------------------------------------------------------
# Omit date column 
hydro<-hydro%>% select(-date)
#Convert catalyst to factor
hydro$catalyst<-as.factor(hydro$catalyst)
```

## Analysis 

### Summary statistics
First, we will have a brief look to the summary statistics of our data. For each catalyst, we note that there are almost exactly the same amount of observations.
```{r,echo=FALSE}
summary(hydro)
```
Next we compute the summarry statistics for the temprature and reactor residence time of the mixture in hours for each catalyst.

```{r,echo=FALSE}
#Summary statistics.
hydro %>%select(catalyst,temperature) %>% group_by(catalyst) %>% 
  summarise(mean=mean(temperature),median=median(temperature),var=var(temperature),
            max=max(temperature),min=min(temperature))%>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")

hydro %>%select(catalyst,through_time) %>% group_by(catalyst) %>% 
  summarise(mean=mean(through_time),median=median(through_time),var=var(through_time),
            max=max(through_time),min=min(through_time))%>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```
We visualise the above results in the figure below.The box plots show the temperature and residence time per catalyst.On the first figure, we observe that the median,maximum and minimum values differ slightly for each catalyst. The distribution seems symmetric and uniform and there aren’t any outliers. 

```{r,echo=FALSE}
#Visualize data --------------------------------------------------------------------
##Through time and Temperature
boxplot_temp<-ggplot(hydro,aes(x=catalyst,y=temperature,color=temperature))+
  geom_boxplot(fill = "#4271AE", colour = "#1F3552",alpha = 0.8)+
  scale_y_continuous(name = "Temperature in\n Fahrenheit",limits=c(400, 1000)) +
  scale_x_discrete(name = "Catalyst")+
  theme_bw()+ ggtitle("Summary of Temperature")+
  theme(plot.title = element_text(size=10),
        text = element_text(size=9))
boxplot_temp

boxplot_time<-ggplot(hydro, aes(x=catalyst, y=through_time, color=through_time)) +
  geom_boxplot()+
  geom_boxplot(fill = "#4271AE", colour = "#1F3552",alpha = 0.8)+
  scale_y_continuous(name = "Summary of Residence Time") +
  scale_x_discrete(name = "Catalyst")+
  theme_bw()+
  labs(title = "Mean residence time")+
  theme(plot.title = element_text(size=10),
        text = element_text(size=9))
boxplot_time
```

### Distribution plots
The distribution graph,below, shows the effluent mixtures for selected density intervals.It can be clearly seen that the variables are normally distributed and as the density interval increase the mean decreases. What is striking is that the variation decreases as well and the variables tend to cluster more around the mean. One reason that can explain this is that as the size and weight of the molecules decrease it is harder to control their proportions. There are also some outliers as the density interval increases.
```{r,echo=FALSE}
##Feed hydrocarbon mixtures
##Feed Yield boxplots --------------------------------------------------
#Data frame of feed mixtures
feed<-hydro %>%select(starts_with('feed'))
# Histogram
#Create dataframe for histogram and selected intervals
hist_feed.data<-feed %>% 
  select(feed_fraction_1,feed_fraction_5,feed_fraction_10,feed_fraction_15,feed_fraction_20) %>% 
  melt(measure.vars = c('feed_fraction_1','feed_fraction_5','feed_fraction_10','feed_fraction_15','feed_fraction_20'))   
#Mean per interval
mu_feed<- hist_feed.data %>% group_by(variable) %>% 
  summarise(mean=mean(value))

hist_feed<-ggplot(hist_feed.data, aes(x=value,color=variable)) + 
  geom_density(position = "identity", alpha=0.2)+
  geom_vline(data=mu_feed,aes(xintercept=mean,color=variable),
             linetype="dashed", size=1)+
  theme_bw()+theme(legend.position='bottom')+
  theme(legend.position = c(0.7, 0.6),
        legend.title = element_text(size=8,face="bold"),
        legend.text = element_text(size=8),
        plot.title = element_text(size=9),
        text = element_text(size=9))+
  xlab('Proportion of Mixture ') +  
  ggtitle('Proportion of Feed Hydrocarbon Mixture\n for selected intervals')+
  labs(colour = "Density interval")+scale_color_brewer(palette="Set1")
hist_feed
```
The next Figures, show the distribution plot of the proportion of the effluent mixture for the same variables and the response variables. Again, we can observe the same trend as above. We see that the mean value for interval 5 have increased slightly, while for interval 1 there is a significant decrease. In particular, for interval 15, there is a sharp increase in the amount of data clustered around the mean, in contrast with interval 20 where the is a sharp decrease.

```{r,echo=FALSE}
##Effluent hydrocarbon mixtures --------------------------------------------------
#Create dataframe
out<-hydro %>%select(starts_with('out'))
#Combined Histogram for effluent mixtures
#Create data for Histogram
hist_out.data<-out %>% 
  select(out_fraction_1,out_fraction_5,out_fraction_10,out_fraction_15,out_fraction_20) %>% 
  melt(measure.vars = c('out_fraction_1','out_fraction_5','out_fraction_10','out_fraction_15','out_fraction_20'))   

#Mean per interval
mu_out<- hist_out.data %>% group_by(variable) %>% 
  summarise(mean=mean(value))
#Histogram
hist_out<-ggplot(hist_out.data, aes(x=value,color=variable)) + 
  geom_density(position = "identity", alpha=0.2)+
  geom_vline(data=mu_out,aes(xintercept=mean,color=variable),
             linetype="dashed", size=1)+
  theme_bw()+
  theme(legend.position = c(0.8, 0.6),
        legend.title = element_text(size=8,face="bold"),
        legend.text = element_text(size=8),
        plot.title = element_text(size=9),
        text = element_text(size=9))+
  xlab('Proportion of Mixture ') +  
  ggtitle('Proportion of Feed Hydrocarbon Mixture for selected intervals')+
  labs(colour = "Density interval")+
  scale_color_brewer(palette="Set1")

hist_out

#Combined Histogram for response variables
#Create data for Histogram
hist_response.data<-out %>% select(out_fraction_13,out_fraction_14,out_fraction_15) %>% 
  melt(measure.vars = c('out_fraction_13','out_fraction_14','out_fraction_15'))
#Find mean
mu_response <-hist_response.data %>%group_by(variable) %>% summarise(mean=mean(value))
#Creat density plot
hist_response<- ggplot(hist_response.data, aes(x=value,color=variable)) + 
  geom_density(position = "identity", alpha=0.2)+
  geom_vline(data=mu_response,aes(xintercept=mean,color=variable),
             linetype="dashed", size=1)+
  theme_bw()+
  theme(legend.position = c(0.8, 0.6),
        legend.title = element_text(size=8,face="bold"),
        legend.text = element_text(size=8),
        text = element_text(size=9))+
  xlab('Proportion of Mixture ') +  
  ggtitle('Response Variables')+
  labs(colour = "Density interval")+
  scale_color_brewer(palette="Set1")
hist_response
```
The next plots show the summary statistics for the effluent mixture in intervals 13,14 and 15 for each catalyst(0,1,2). Closer inspection of the boxplots shows that there is an increase in proportion when using catalyst 1 and 2 opposed to catalyst 0.There isn’t a significant difference between catalysts 1 and 2 as the different intervals seem to have the same median and range in the two cases.

```{r,echo=FALSE}
#Box plot catalyst, response variables and sum.
#Create response variable ---------------------------------------------------------------------------------------------------
df<-hydro %>% 
  mutate(sum_int = out_fraction_13+out_fraction_14+out_fraction_15)
#Create Data frame 
out_catalyst.data <-df %>% 
  select(catalyst,out_fraction_13,out_fraction_14,out_fraction_15) %>% 
  pivot_longer(-catalyst, names_to = "variable", values_to = "value")
#Box plots
out_catalyst<-ggplot(out_catalyst.data,aes(x=variable,y=value,color=variable))+
  geom_boxplot()+facet_grid(~catalyst)+
  theme_bw()+
  theme(text = element_text(size=12),strip.text.x = element_text(size = 12),legend.position = c(0.9, 0.8))+
  scale_x_discrete(name = "Density Intervals",label=c('13','14','15'))+
  ggtitle('Mixture Proportion for Response Variables for each catalyst ')+
  labs(colour = "Density interval")+
  scale_color_brewer(palette="Set1")
out_catalyst
```
The scatter plots below show the relationship between temperature (left), residence time(right) and
mixture proportions for interval 13, 14, 15 as well as their sum. The lines represent the regression line which was calculates using the LOESS method.In short, this method calculates the regression line taking into account only the nearby points, in contrast with a standard linear regression model which uses all points.As shown in the last graph, the sum of proportions remains relatively steady while the temperature increases.There is a slight decrease in proportion between 400◦F and 680◦F , but it seems that temperature doesn’t dramatically affect the response variables. The right graph reveals that there has been a gradual increase in the proportion as the hours showing the residence time of the mixture in the reactor increase.
 
```{r,echo=FALSE}
#Scatter plot temperature, response variables and sum.
out_temp<-df %>% 
  select(temperature,out_fraction_13,out_fraction_14,out_fraction_15,sum_int) %>% 
  pivot_longer(-temperature, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x=temperature,y=value , colour = variable)) + 
  geom_point(alpha=0.4)+
  geom_smooth(method = "loess",se=F)+
  theme_bw()+
  theme(legend.position = c(0.7, 0.8),
        legend.title = element_text(size=8,face="bold"),
        legend.text = element_text(size=8),
        text = element_text(size=9))+
  xlab("Temperature(°F)") +  
  ylab('Proportion of Mixture')+
  ggtitle('Temperature and Mixture Proportion')+
  labs(colour = "Density interval")+
  scale_color_brewer(palette="Set1")
out_temp 

#Scatter plot time, response variables and sum.
out_time<-df %>% 
  select(through_time,out_fraction_13,out_fraction_14,out_fraction_15,sum_int) %>% 
  pivot_longer(-through_time, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x=through_time,y=value, colour = variable)) + 
  geom_point(alpha=0.4)+
  geom_smooth(method = "loess",se=F)+
  theme_bw()+
  theme(legend.position = c(0.7, 0.8),
        legend.title = element_text(size=8,face="bold"),
        legend.text = element_text(size=8),
        text = element_text(size=9))+
  xlab("Residence Time(hours)") +  
  ylab('Proportion of Mixture')+
  ggtitle('Residence Time and Mixture Proportion')+
  labs(colour = "Density interval")+
  scale_color_brewer(palette="Set1")
out_time
```
# Prediction
Having explored our variables, the following section will discuss the methods used to determine which of the control variables are most influential on the yield of hydrocarbon in the target range of densities. First,we will determine which variables to use as predictors and how to model the response variable. Moving on, we will first compute the correlation matrix to get an initial idea of the relationship between the variables. Machine learning models are often utilised for feature ranking, as some models inherently rank features internally or it’s easy to generate and access the ranking from the structure of the model. In summary we’re going to explore Regularisation Methods, Random Forests, Boosting Machines and Partial Least Squares regression(PLS).

## Correlation Matrix
The correlation matrix is used to determine if a relationship exists between the variables. It consists of the Pearson correlation coefficients for a set of variables and shows the degree of the relationship as well as the direction. We will also calculate the significance levels (p-values) to identify whether the correlation between variables is significant.

```{r,echo=FALSE}
#Correlation matrices--------------------------------------------------------------------------------------
#Prepare data
features<-df[,2:23]
features<-cbind(features,sum=df$sum_int)
colnames(features)[3:22]<-as.character(seq(1:20))
colnames(features)[1]<-'temp'
colnames(features)[2]<-'time'

#Find correlation matrix
cor.features<-cor(features)

#Visualize correlations
plot.cor<-corrplot(cor.features,  method = "color")

#Find the significance of correlations between variables
matrix_feed<-rcorr(as.matrix(features))

#Flatten matrix for visualization
cor_matrix<-flattenCorrMatrix(matrix_feed$r, matrix_feed$P)
cor_matrix[,3:4]<-round(cor_matrix[,3:4],2)

#Create table with significant correlations
sign.cor<-rbind(cor_matrix[232:233,],cor_matrix[243:249,])
sign.cor%>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

```{r,echo=FALSE}
#Create bar chart for Important features
cor.imp<-cor_matrix %>% 
  #Filter response variable
  filter(column=='sum') %>% 
  ggplot(aes(x = abs(cor),
             y = forcats::fct_reorder(.f = row, .x = abs(cor)),fill =abs(cor))) +
  geom_col() +
  ggtitle('Correlation Matrix') +
  xlab("Coefficients")+
  ylab(NULL)+
  theme_bw()+
  scale_color_brewer("Set1")+
  theme(legend.position = "none")
```
The plot shows the correlation between the features and the response variable (sum). The table summarises the most important coefficients as well as their significance levels. We can see that there is a strong correlation between the sum,the time and the feed mixtures in interval 12 to 15. Their p-values are less than the significance level of 0.05, which indicates that the correlation coefficients are significant. There is a low degree of correlation between the re- sponse variable and temperature , and the p-value indicates that the coefficient is not very significant.

## Prepare Training And Test data sets
Before moving to the methods use, we need to determine the response variable. As mentioned above, the hydrocarbons of interest are within the 13,14 and 15 density range.This study assumes that the effluent mixtures don’t interact with each other and consequently they are omitted. We want to find the combination of variables that result in the maximum yield in intervals 13,14 and 15 and as a result, our response variable will be the sum of the mass proportions. The features or predictors used will be the variables catalyst, temperature, residence time and proportions of feeding mixtures in different intervals. Moreover, we will split the data and use 80% of the data for training and 20% for testing. In order to reduce the variance further we will use a 10-fold validation.

```{r}
#Prepare Training And Test Data----------------------------------------------------------------------------
#Create response variable
hydro<-hydro %>% 
  mutate(sum_int = out_fraction_13+out_fraction_14+out_fraction_15)
#Define data and remove features we don't need
data<-hydro %>% select(-starts_with('out'))
#Define training indexes
trainingIndex <- sample(1:nrow(data), 0.8*nrow(data)) # indices for 80% training data
#Define training data
data.train<-data[trainingIndex, ]
#Define testing data
data.test <- data[-trainingIndex, ]
```

## Regularisation Methods
Regularization is a method that reduces overfitting of a model by adding penalty to a model. The most popular regularization methods are lasso and ridge regression, which use a penalty term λ to regularize the coefficients such that the optimization function is penalized if the coefficients are too large. The most important features will have the the highest coefficients in the model, while features that don’t affect the result as much should have coefficient values close to zero.

In this approach, we are going to implement Elastic net regression model which is a combination of the two above methods and incorporates penalties from both. This is achieved by setting the alpha parameter $\alpha$ , which determines the degree of mixing between ridge regression and lasso: when $\alpha$ is set to zero, the model corresponds to ridge regression , and when $\alpha=1$ , it corresponds to lasso. Values of $\alpha$  in between will result to a blend of the two.

In order to determine the optimal value for $\alpha$ , we’re going to test different values and choose the one that results in the minimum Mean Squared Error(MSE). In addition, we will use 10-fold Cross Validation to determine the optimal value for $\lambda$  for each value of $\alpha$.The parameter $\lambda$ is called the shrinkage parameter;when $\lambda=0$, there is no shrinkage performed and as it increases the coefficients are shrunk more strongly. This happens regardless of the value of $\alpha$. We’re going to use the lambda that results in the model with the minimum cross validation error and test values of $\alpha$ that range from 0 to 1 with a step equal to 100.

### Elastic Net Regression

```{r}
##Regularization----------------------------------------------------------------------------
#Create observations and target variable for training data
x.train<- data.train %>% select(-sum_int) %>% data.matrix
y.train<- data.train$sum_int# training data
#Create observations and target variable for testing data
x.test<-data.test %>% select(-sum_int) %>% data.matrix
y.test<- data.test$sum_int

#Create empty list used to save fits
list.of.fits <- list()
#number of iterations
it<-100
## We are testing alpha = i/100.
for (i in 0:it) {
  ## Create variable name
  fit.name <- paste0("alpha", i/it)
  ## Fit a model and store it in a list 
  list.of.fits[[fit.name]] <-
    cv.glmnet(x.train, y.train, type.measure="mse", alpha=i/it, 
              family="gaussian")
}
## Find which alpha results in the min MSE
reg.results <- data.frame()
for (i in 0:it) {
  fit.name <- paste0("alpha", i/it)
  ## Use each model to calculate predictions given the Testing dataset
  predicted <- 
    predict(list.of.fits[[fit.name]], 
            s=list.of.fits[[fit.name]]$lambda.min, newx=x.test)
  
  ## Calculate the Mean Squared Error
  mse <- mean((y.test - predicted)^2)
  
  ## Store the results
  temp <- data.frame(alpha=i/it, mse=mse, fit.name=fit.name)
  reg.results <- rbind(reg.results, temp)
}

#See results------Uncomment to see table
#reg.results%>% knitr::kable("html") %>%
#  kableExtra::kable_styling(latex_options = "hold_position")
#Plot mse and alpha value 
ggplot(reg.results,aes(x=alpha,y=mse))+
  theme_bw()+
  geom_line(color="#1F3552")+
  ggtitle("MSE for different alpha values")+
  ylab("MSE")
```

```{r,echo=FALSE}
#Save best alpha
best.alpha=reg.results$alpha[which.min(reg.results$mse)]
#Use the value to create the best model
reg.model<-cv.glmnet(x.train, y.train, type.measure="mse", 
                       alpha=best.alpha, 
                       family="gaussian")

#Find non zero coefficients
coef.reg <- coef(reg.model)
#Create data frame
imp.coef_reg<-data.frame(coef=coef.reg[coef.reg[,1]!=0,])
imp.coef_reg<-rownames_to_column(imp.coef_reg, "variables")
imp.coef_reg$variables<-str_replace(imp.coef_reg$variables,"feed_fraction_", "")

#Plot most important variables
reg_coef<-imp.coef_reg %>%
  filter(variables!=c("(Intercept)")) %>% 
  ggplot(aes(x = abs(coef),y = forcats::fct_reorder(.f = variables, 
                                    .x = abs(coef)),fill =abs(coef))) +
  geom_col() +
  ggtitle('Elastic Net Regression') +
  xlab("Coefficients")+
  ylab(NULL)+
  theme_bw()+ 
  scale_color_brewer("Set1")+
  theme(legend.position = "none")

#use fitted best model to make predictions
reg.predicted <- predict(reg.model,alpha = best.alpha, newx = x.test)

#Performance metrics
#find SST and SSE
reg.sst <- sum((y.test - mean(y.test))^2)
reg.sse <- sum((reg.predicted - y.test)^2)
reg.mse <- mean((y.test - reg.predicted)^2)
#find R-Squared
reg.rsq <- 1 - reg.sse/reg.sst

#Data frame with performance metrics
reg.perf=data.frame(MSE=reg.mse, 
                    Rsquare=reg.rsq*100,
                    RMSE=sqrt(reg.mse))
```

### Lasso Regression
```{r}
#Lasso model----------------------------------------------------------------------------
#Create vector with lambdas
lambdas <- 10^seq(2, -3, by = -.1)
# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x.train, y.train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)
# Best model
lambda_best <- lasso_reg$lambda.min 
lasso_model <- glmnet(x.train, y.train, alpha = 1, lambda = lambda_best, standardize = TRUE)

#Save coefficients
coef.lasso<- coef(lasso_model)
#Compare coefficients with Lasso model
cbind(coef.reg, coef.lasso)

#Create data frame
imp.coef_las<-data.frame(coef=coef.lasso[coef.lasso[,1]!=0,])
#Convert row names to column
imp.coef_las<-rownames_to_column(imp.coef_las, "variables")
imp.coef_las$variables<-str_replace(imp.coef_las$variables,"feed_fraction_", "")
```

```{r,echo=FALSE}
#Plot most important variables
las_coef<-imp.coef_las %>%
  filter(variables!=c("(Intercept)")) %>% 
  ggplot(aes(x = abs(coef),y = forcats::fct_reorder(.f = variables, 
                                                    .x = abs(coef)),fill =abs(coef))) +
  geom_col() +
  ggtitle('Lasso Regression') +
  xlab("Coefficients")+
  ylab(NULL)+
  theme_bw()+
  scale_color_brewer("Set1")+
  theme(legend.position = "none")
```
```{r}
#Plot mse and alpha value 
ggplot(reg.results,aes(x=alpha,y=mse))+
  theme_bw()+
  geom_line(color="#1F3552")+
  ggtitle("MSE for different alpha values")+
  ylab("MSE")
```
The $\alpha$ that minimises the MSE is equal to 0.02. It’s interesting to note the best solution is very close to Ridge Regression but when  $\alpha = 0$ the model performs the worst out of all tested models. The Lasso model$(\alpha= 1)$ is located in between in terms of performance.
```{r,echo=FALSE}
#Prediction using model
lasso.pred<-predict(lasso_reg,lambda = lambda_best, newx = x.test)
#Performance metrics
#find SST and SSE
lasso.sse <- sum((lasso.pred - y.test)^2)
lasso.mse <- mean((y.test - lasso.pred)^2)
#find R-Squared
lasso.rsq <- 1 - lasso.sse/reg.sst

#Data frame with performance metrics
lasso.perf<-data.frame(MSE=lasso.mse, 
                       Rsquare=lasso.rsq*100,
                       RMSE=sqrt(lasso.mse))
lasso.perf
```
## Random Forest
Moving on now to consider random forest method, which is another method used for both prediction and feature importance estimation.The method uses multiple decision trees and combines the resulted predictions to create a more accurate model. Random Forests split the data into training and test sets by default since each tree is trained using a different dataset randomly sampled from the original data.Moreover, they are often used to find the variables with the most predictive power by measuring the Node purity and how much the accuracy decreases when the variable is excluded. Random Forests are prone to overfitting and as a result we will need to find the best hyperparameters for our model and iterate through different options. Once the process is done, we train the best model and find the most important features based on mean decrease impurity.Then, we remove features from the bottom of the list and evaluate the impact on performance.

```{r}
##Random forest -----------------------------------------------------------------------------------------------------------------------
#Evaluate different values for ntree.
#Create empty list for saving performance and number of trees
modellist <- data.frame()
#Loop through different number of trees
for (ntree in c(500,1000, 1500, 2000, 2500)) {
  set.seed(1)
  key <- toString(ntree)
  fit <- randomForest(sum_int ~ ., 
                      data=data,
                      ntree=ntree,
                      importance=TRUE,
                      xtest   = data.frame(x.test),
                      ytest   = y.test)
  modellist<- rbind(modellist,c(ntree,sqrt(fit$mse[which.min(fit$mse)])))
}
#Format column names
colnames(modellist)<-c('ntrees','MSE')

#Find best number of trees
best.ntree<-modellist[which.min(modellist$MSE),1]

#Train best model
rf.model0 <- randomForest(sum_int ~ ., 
                          data=data,
                          ntree=best.ntree,
                          importance=TRUE,
                          xtest   = data.frame(x.test),
                          ytest   = y.test)
```

```{r,echo=FALSE}
#Prediction accuracy
y.pred0<-rf.model0$test$predicted
mean<-sum(y.test)/length(y.test)
rf.mse0<-sum((y.test-y.pred0)^2)/length(y.test)
rf.r20<-(1-(rf.mse0/sum((y.test-mean)^2)/length(y.test)))*100
#Create data with performance analytics
rf.perf0<-data.frame(MSE=rf.mse0,
                     Rsquare=rf.r20,
                    #find RMSE of best model
                    RMSE=sqrt(rf.model0$mse[which.min(rf.model0$mse)]) )

# Get variable importance from the model fit
ImpData <- as.data.frame(importance(rf.model0))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )

ImpData$Var.Names<-str_replace(ImpData$Var.Names,"feed_fraction_", "")

#Plot most important variables
rf_coef0<-ImpData %>%
  ggplot(aes(x = `%IncMSE`,y = forcats::fct_reorder(.f = Var.Names, 
                                                    .x = `%IncMSE`),fill =`%IncMSE`)) +
  geom_col() +
  ggtitle('Random Forest') +
  xlab("%IncMSE")+
  ylab(NULL)+
  theme_bw()+
  scale_color_brewer("Set1")+
  theme(legend.position = "none")
```
Next we chose the best feautures according to the model above and train a new model.

```{r,echo=FALSE}
#Extract important features to train new model
imp_var<-ImpData[which(abs(ImpData$`%IncMSE`)>10),]

#Create new data frame with important variables
dat.imp<-data %>% select(rownames(imp_var),sum_int)
#Testing data
x.test.imp<-data.test %>% select(rownames(imp_var))
#Find optimum number of trees
modellist1 <- data.frame()
for (ntree in c(500,1000, 1500, 2000, 2500)) {
  set.seed(1)
  key <- toString(ntree)
  fit <- randomForest(sum_int ~ ., 
                      data=dat.imp,
                      ntree=ntree,
                      importance=TRUE,
                      xtest   =  data.frame(x.test.imp),
                      ytest   = y.test)
  modellist1<- rbind(modellist,c(ntree,sqrt(fit$mse[which.min(fit$mse)])))
}
colnames(modellist1)<-c('ntrees','MSE')
best.ntree1<-modellist1[which.min(modellist1$MSE),1]

#Train best model
rf.model1 <- randomForest(sum_int ~ ., 
                          data=dat.imp,
                          ntree=best.ntree1,
                          importance=TRUE,
                          xtest   = data.frame(x.test.imp),
                          ytest   = y.test)


# Get variable importance from the model fit
ImpData1<- as.data.frame(importance(rf.model1))
ImpData1$Var.Names <- row.names(ImpData1)
ImpData1$Var.Names<-str_replace(ImpData1$Var.Names,"feed_fraction_", "")


ggplot(ImpData1, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
#Plot most important variables
rf_coef1<-ImpData1 %>%
  ggplot(aes(x = `%IncMSE`,y = forcats::fct_reorder(.f = Var.Names, 
                                                    .x = `%IncMSE`),fill =`%IncMSE`)) +
  geom_col() +
  ggtitle('Random Forest(Reducted)') +
  xlab("%IncMSE")+
  ylab(NULL)+
  theme_bw()+
  scale_color_brewer("Set1")+
  theme(legend.position = "none")
```

The minimum MSE is achieved by the model with a number of trees equal to 2000. After training our model with the new hyperparameters we extract the features with Importance level greater than 10%. Those features are feed mixtures from density interval 2,12,13,14 and 15 as well as catalyst and through time (Figure 7). After, we find the best number of trees, which is equal 2500, we train the new model.
```{r,echo=FALSE}
#Prediction accuracy
y.pred1<-rf.model1$test$predicted
rf.mse1<-sum((y.test-y.pred1)^2)/length(y.test)
rf.r21<-(1-(rf.mse1/sum((y.test-mean)^2)/length(y.test)))*100
rf.perf1<-data.frame(MSE=rf.mse1, 
                     Rsquare=rf.r21,
                     RMSE=sqrt(rf.model1$mse[which.min(rf.model1$mse)]))
rf.perf1 %>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

## Gradient Boosting Machines (GBMs)

Random forests as mentioned above build an ensemble of deep independent trees while Gradient Boost Machines (GBMs) train trees sequentially.This means that each tree learns and improves on the previ- ous.An important feature of GBMs is the Variable Importance, which ranks the feature variables based on the importance of each in training the model. GBMs are also prone to overfitting and thus we will need to find the best hyperparameters and train the model based on the best set.

```{r}
#Gradient Boosting Machine-----------------------------------------------------------------------------------------------------------------------
#Fit model
boost_feed <- gbm(sum_int ~ ., data=data.train,
                  distribution = "gaussian", 
                  n.trees = 2000, 
                  shrinkage = 0.01,
                  interaction.depth = 4,
                  cv.folds = 10)
#Summary
summary(boost_feed)
#Find the number of trees that results to the minimum square error
gbm.perf(boost_feed,method = 'cv')
which.min(boost_feed$cv.error)
```
The default settings in gbm includes a learning rate, which is used to reduce the impact of each addi- tional tree.In this case, we use a learning rate of 0.01 to avoid overfitting. Next, we use a number of trees of 2000 with depth of each tree equal to 4. Lastly, we specify a gaussian distribution, scale our data and perform a 10 fold cross validation. The results show that our MSE loss function is minimized with 1901 trees, which is used to compute our predictions.

```{r}
#Compute predictions using the optimum number of trees
boost_pred <- predict(
  boost_feed, 
  newdata = data.test,ntrees=which.min(boost_feed$cv.error))

#Performance metrics
bg.mse<-sum((data.test$sum_int-boost_pred)^2)/length(data.test$sum_int)
bg.r2<-(1-(rf.mse0/sum((data.test$sum_int-mean)^2)/length(data.test$sum_int)))*100
bg.perf<-data.frame(MSE=bg.mse,
                    Rsquare=bg.r2,
                    RMSE= sqrt(mean((boost_pred - data.test$sum_int)^2)))

#Find influential variables 
FeedEffects <- as_tibble(summary.gbm(boost_feed,plotit = FALSE))
#Format variables names
FeedEffects$var<-str_replace(FeedEffects$var,"feed_fraction_", "")

#Plot important variables 
boost_coef<-FeedEffects %>% 
  # plot these data using columns
  ggplot(aes(x = forcats::fct_reorder(.f = var, 
                                      .x = rel.inf), 
             y = rel.inf, 
             fill = rel.inf)) +
  geom_col() +
  # flip
  coord_flip() +
  # format
  theme_bw() +
  scale_color_brewer("Set1")+
  theme(legend.position = "none")+
  xlab('') +
  ylab('Relative Influence') +
  ggtitle("Gradient Boosting Machine")

# Create Data frame with predicted and actual values
boost.results<-data.frame(actual=data.test$sum_int,predicted=boost_pred)

# plot predicted v actual
ggplot(boost.results) +
  geom_point(aes(y = predicted, 
                 x = actual, 
                 color = predicted - actual), alpha = 0.7) +
  # add theme
  theme_bw() +
  # strip text
  theme(axis.title = element_text()) + 
  # add axes/legend titles
  scale_color_continuous(name = "Predicted - Actual") +
  ylab('Predicted Effluent Mixture') +
  xlab('Actual Effluent Mixture') +
  ggtitle('Predicted vs Actual') 
```


## Partial Least Squares Regression(PLS)
Partial least squares regression is a method that was developed to solve the problem of possibly cor- related feature variables, and relatively few samples.PLS uses a dimension reduction strategy that is supervised by the outcome.This means that it identifies new principal components that describe as much as possible the co-variance between the feature and response variables. These components are then used to fit the regression model. Principal Component Analysis (PCA) finds a way to reduce the dimensions of the data and the principal components are new variables that are constructed as linear combinations or mixtures of the initial variables.The number of principal components is chosen by cross-validation.

```{r}
##Partial Least Squares Regression --------------------------------------------------------------------------------------------
# Build the model on training set
pls.model <- train(
  sum_int~., data = data.train, method = "pls",
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
## Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
summary(pls.model)

# Plot model RMSE vs different values of components
pc_perf<-data.frame(ncomp=pls.model$results$ncomp, rmse=pls.model$results$RMSE, rsquared=pls.model$results$Rsquared)
rmse.pls<-ggplot(pc_perf,aes(x = ncomp,y = rmse)) +
  geom_line()+
  # format
  scale_color_brewer(palette = "Dark2") +
  theme_bw() + 
  scale_x_continuous(n.breaks = 10)+
  xlab('Number of Compoments') +
  ylab('RMSE(cross-validation') +
  ggtitle("RMSE for each component")

rsquared.pls<-ggplot(pc_perf,aes(x = ncomp,y = rsquared)) +
  geom_line()+
  # format
  scale_color_brewer(palette = "Dark2") +
  theme_bw() + 
  scale_x_continuous(n.breaks = 10)+
  xlab('Number of Compoments') +
  ylab('Rsquared') +
  ggtitle("Rsquared for each component")

grid.arrange(rmse.pls,rsquared.pls,ncol=2)

# Compute predictions
pls.pred <- predict(pls.model,data.test)
# Model performance metrics
pls.mse<-sum((data.test$sum_int-pls.pred)^2)/length(data.test$sum_int)
pls.perf<-data.frame(
  MSE=pls.mse,
  Rsquare = caret::R2(pls.pred, data.test$sum_int)*100,
  RMSE = caret::RMSE(pls.pred, data.test$sum_int))

```

For the last method, we choose to scale our data again and use a 10-fold cross-validation to evaluate the performance of the model.Once we’ve fit the model, we determine the number of PLS components we want to keep.Looking at figure 7, we see that after 2 components there isn’t a significant change in the RMSE and R2, and hence we could use only the first two components for the optimal model.

# Final Results
The plots below presents the feature importance as computed by all methods. It is interesting to note that in all methods used in this study, the most influential variable is the feed hydrocarbon mixture in density interval 13. Another important finding was is that the feed hydrocarbon mixture in density intervals 12 and 14 are the second and third most influential followed very closely by molecules in interval 15. It is somewhat surprising that the Random Forests found that catalyst is more influential than feed mixture in interval 15 as it’s contradicting the results of the other methods. In addition, Elastic net and Lasso regression identify feed mixtures in intervals 10 and 11 as more important than catalysts, residence time and temperature. However, the Lasso model still considers those variables important and didn’t shrink the coefficients to zero.
In comparison, the methods based on decision trees i.e Random Forest and GBMs, consider the reactor’s setting as significant. Moreover, we can see that temperature doesn’t affect the results significantly which confirms our observations in the explanatory analysis. All methods show that the choice of catalyst is more important than the residence time and temperature. Finally, we can say with some certainty that the feed mixtures that are not close to the interval ranges of interest are not influential.
```{r}
### Final metrics --------------------------------------------------------------------------------------------
#Important Features 
grid.arrange(cor.imp,reg_coef,las_coef,rf_coef0,rf_coef1,boost_coef,nrow=3,ncol=2)
```
Turning now to the prediction performance of each model.The graphs below shows the mean square error and R2 plots for every method.The table in the right shows the exact metrics. We observe the random forest model trained all the variables,achieves the minimum mean square error equal to MSE = 1.21 · 10−5 and best R2 equal to 99.9%. The following best model is the random forest trained on the important variables with MSE = 1.25 · 10−5 and R2 = 99.9%. Next the Partial Least Square Regression, has a small MSE of 3.81 · 10−5 and lower R2 equal to 82%. The Elastic Net regression has the worse R2 equal to 80.18% and a quite high MSE, equal to 3.83 · 10−5. Following the Lasso model with MSE equal to 4.23 · 10−5 and R2 equal to 78.10%. Finally, The GBM model performs the worse with a R2 equal to 99% but a quite high MSE equal to 4.736 · 10−5.

```{r,echo=FALSE}
### Final metrics --------------------------------------------------------------------------------------------
#Create data Frame containing all the perfomance metrics
perfomance<-rbind(reg.perf,lasso.perf,rf.perf0,rf.perf1,bg.perf,pls.perf)
rownames(perfomance)<-c('ElasticNet','Lasso','RF0','RF1', 'GBM','PLS')
#Convert rownames to column
perfomance<-rownames_to_column(perfomance, "Methods")
perfomance %>% knitr::kable("html") %>%
  kableExtra::kable_styling(latex_options = "hold_position")

#Plot results
#MSE
total_mse<-perfomance %>% 
  # plot these data using columns
  ggplot(aes(x = forcats::fct_reorder(.f = Methods, 
                                      .x = MSE), 
             y = MSE,fill = MSE)) +
  geom_col() +
  # flip
  coord_flip() +
  # format
  scale_color_brewer(palette = "Dark2") +
  theme_bw() +
  theme(axis.title = element_text(),legend.position="none") + 
  xlab('Methods') +
  ylab('Mean Square Errror') +
  ggtitle("Mean Square Error")

#Rsquare
total_Rsquare<-perfomance %>% 
  # plot these data using columns
  ggplot(aes(x = forcats::fct_reorder(.f = Methods,.x = Rsquare), 
             y = Rsquare,fill = Rsquare)) +
  geom_col() +
  # flip
  coord_flip() +
  # format
  scale_color_brewer(palette = "Dark2") +
  theme_bw() +
  theme(axis.title = element_text(),legend.position = "none") + 
  xlab('Methods') +
  ylab('Rsquared (%)') +
  ggtitle("Rsquared")
grid.arrange(total_mse,total_Rsquare,ncol=2)

```

